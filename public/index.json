
[{"content":"","date":"28 November 2024","externalUrl":null,"permalink":"/tags/example/","section":"Tags","summary":"","title":"Example","type":"tags"},{"content":" Your browser does not support the video tag. Platform: Oculus Quest Engine: Unity Genre: Sales Training Team: 1 Engineer(me), Additional work by 1 Engineer, 1 artist/designer, 1 project manager\nDescription: This was another VR game made for the same purpose as FOP Sam. this time, we targeted a different set of products and a different set of potential customers to feature in the game. As before, the idea is to show a few interactive scenes of the future of pet care, prominently featuring products only just starting to be developed today, and to get marketing and sales specialists aquainted with the vision of the company. This pet care company was a client of RazorEdge LLC, a VR-focused serious games company that I did intermittent contract work with from 2021-2023.\nEngineering Tasks:\nGAMEPLAY SYSTEMS PROGRAMMING\nI took on a more central role this time as the engineer on this project, with razoredge\u0026rsquo;s other engineer only helping a little bit. As before, I programmed all of the gameplay that occurred in between the timeline sections, including the digital interfaces, \u0026lsquo;virtual\u0026rsquo; vr headset, and interactions with the dog character.\nUNITY TIMELINE SEQUENCING AND ART PIPELINE\nI configured all of the content in the game as well, mostly making use of unity timeline and figma. I had more work to do in figma this time as our client\u0026rsquo;s brand identity requirements were abit more strict. I ended up handling most of the 2d art pipeline myself. However, when working on the 3d art pipeline, there was a last minute emergency issue with animations on the dog character! I was able to make it work with unity timeline anyway, however I now know that sometimes unity timeline doesn\u0026rsquo;t treat different animation\u0026rsquo;s offsets the same way.\n","date":"28 November 2024","externalUrl":null,"permalink":"/projects/1732755138149-future-of-pet---maria/","section":"Projects","summary":"","title":"Future of Pet - Maria","type":"projects"},{"content":" Your browser does not support the video tag. Platform: Oculus Quest Engine: Unity Genre: Sales Training Team: 2 Engineers (me), 1 artist/designer, 1 animator, 1 project manager\nDescription: This was a VR game made to familiarize employees at a pet care company about the roles and future vision of product lines in their company. The idea is to show a few interactive scenes of the future of pet care, prominently featuring products only just starting to be developed today, and to get marketing and sales specialists aquainted with the vision of the company. This pet care company was a client of RazorEdge LLC, a VR-focused serious games company that I did intermittent contract work with from 2021-2023.\nEngineering Tasks:\nGAMEPLAY SYSTEMS PROGRAMMING\nI programmed all of the gameplay in FOP Maria, but it was through the excellent tools development of my engineer colleague that I was able to do so. He had already made a tool pipeline for the team long before I joined, and there were many unity timeline - based utilities that he made that made my life a lot easier! Any non- timeline based gameplay in the game, such as the virtual phone screen, I programmed myself as normal.\nUNITY TIMELINE SEQUENCING\nI also set up all the content in the game, manually adjusting all animation and timeline sequences to match VO and coordinate seamlessly with moments of player-driven pacing in gameplay.\nEDITOR SCRIPTING\nI extended some of the tools to make my own timeline tasks a bit easier to manage, such as automatically adjusting animation offsets and rootmotion at edit time.\nOther Tasks:\nART PIPELINE COORDINATION\nI struggled again on this project to get all the animation I needed in the right format. I made sure to communciate more effectively this time though, and leveraged what I had learned last time about animation rigging and the 3d humanoid workflow in blender to better understand the problem from the artist side. In the end, the artist and I were able to work together much better and get better looking animations into the project in less time.\n","date":"28 November 2024","externalUrl":null,"permalink":"/projects/1732755104808-future-of-pet--sam/","section":"Projects","summary":"","title":"Future of Pet- Sam","type":"projects"},{"content":"","date":"28 November 2024","externalUrl":null,"permalink":"/","section":"Great Tree Games","summary":"","title":"Great Tree Games","type":"page"},{"content":"","date":"28 November 2024","externalUrl":null,"permalink":"/tags/mobile/","section":"Tags","summary":"","title":"Mobile","type":"tags"},{"content":"Platform: PC Engine: Unity Genre: Various (adventure, platformer, management game) Team: 1 Engineer and Designer(me), 1 Narrative Designer, 1 Artist, 1 Project Lead/Design Lead/Producer\nDescription: I met with a team of creatives at GDC one year who were interested in adapting the work of an author friend of theirs into a climate fiction game. With my ambitions at the time to break into paid work in the industry, and my former life as an environmental science professional, it was a natural fit. I worked contracts consistently for several months, then more sporadically over the following year with their team. Eventually, the team dissolved due to legal issues with the founder, but not before I was able to produce and give design input on a number of diverse prototypes and participate in the concepting process with the 2d artist and narrative designer. I also got the chance to meet and work with some design consultants from Playdead (developers of LIMBO and INSIDE) and keithBurgungames.\nDesign Tasks:\nDESIGN DOCUMENTATION\nI coordinated design meetings between the project\u0026rsquo;s visionholder, artist and narrative designer, keeping track of and updating design documentation as the project evolved.\nSTAKEHOLDER INTERVIEWS\nI also did interviews of both the visionholder and narrative designer on design goals and technical needs for both tools and prototypes on the project. I helped to constrain overscoping and also to help put schema and structure to the narrative content so it would fit a future narrative pipeline for the game.\nINTERNAL PITCHING\nIn addition to listening and helping with design documentation, scope and tools requirements, I also was able to pitch some of my own gameplay ideas during certain phases of the project. I was also able to use my design knowledge to help the visionholder identify analogues to his gameplay ideas in published projects to help derive further insights for gameplay programming.\nMOCKUPS\nI mocked some of my design ideas as well as some of the concepts on the design document using figma.\nCONTENT DEVELOPMENT AND NARRATIVE CONTENT PIPELINE COORDINATION\nOver the course of the project, I ended up working more and more closely with the narrative designer, both so that she could understand the new medium she was entering into, and so that I could understand her needs for content pipeline and how to structure and parametrize the content she was writing for the game.\nEngineering Tasks:\nGAMEPLAY SYSTEMS PROGRAMMING AND SOFTWARE ARCHITECTURE\nI programmed all the gameplay systems in all of the prototypes myself. Since the vision for the game was changing rapidly, I needed to make my code as modular and robust as possible to be able to reuse as much of it as I could. I was particularly proud of my internal architecture on the idle game -like prototype, I created a lot of highly generalizable systems to create very diverse content ingame in a no-code fashion.\n3D UI/UX PROGRAMMING\nFor the 3d globe / orrery prototype, I did a lot of complex math to ensure the dynamic camera and worldspace-based UI would line up properly regardless of edge cases. We also had accurate distances between planets in this scene, because it wasn\u0026rsquo;t yet known if we would have a section where the player would directly control spacecraft. So I was able to do that while accounting for floating point precision limitations.\nTOOLS DEVELOPMENT\nI did extensive tools development on this project. We kept changing the narrative tools we were using to create the content- we went from articy:draft, to inkle writer, to twine, to dialoguesystem for unity, to a custom system. I had to be pretty fast to adapt to changing requirements on the tool integration side, with my own custom adapters and similar features. I created a system to dynamically parse and restructure inkle script using regex and winforms while ingesting it into the unity project assets. I also made various editor utilities, and csv ingestion functionality.\nPLUGIN INTEGRATION\nIn addition to changing the external tool setup rapidly, I was also routinely asked to integrate large sections of store-bought asset code quickly. It was a challenge to learn the ins and outs of so many competing code structures and integrate them all without compatibility problems, so I developed a very module-based coding style to accomodate the changes.\n","date":"28 November 2024","externalUrl":null,"permalink":"/projects/1732755188002-project-sentinel/","section":"Projects","summary":"","title":"Project Sentinel","type":"projects"},{"content":"","date":"28 November 2024","externalUrl":null,"permalink":"/projects/","section":"Projects","summary":"","title":"Projects","type":"projects"},{"content":" Your browser does not support the video tag. Platform: Oculus Engine: Unity Genre: Sales Training, Dialogue Based Team: 1 Engineer (me), 1 3d Artist and Project Manager, content writers from elsewhere in the company\nDescription: I was the sole engineer and developer on this VR sales training app that I completed for Pulse LLC, who in turn was working under contract with a top 5 tech company to create this serious game. I worked with the artist and project manager to create a dialogue based interactive sales training experience using Dialogue System for Unity as well as a bunch of different oculus-specific unity toolkits such as horizon worlds avatars. The goal of the game\u0026rsquo;s creation was to produce an immersive retail sales training experience to get frontline staff used to sales techniques they would be expected to know. We also had ingame streaming video from the company website, and telemetry to track each user\u0026rsquo;s progress and reflect it ingame from an online account system.\nDesign Tasks:\nFOCUS TESTING\nI didn\u0026rsquo;t have much design input on this game, but the one designer task I did do was focus testing. I carefully paid attention and took notes on how playtesters interacted with content, and evaluated how closely it aligned with learning goals. I took note of confusion points and bugs.\nEngineering Tasks:\nGAMEPLAY SYSTEMS PROGRAMMING\nI programmed all the gameplay in the game, adapting content from oculus-recommended toolkits where needed and handrolling the rest. I made the navigation system and connected all world state logic to dialoguesystem for unity, and all avatar state and animation programming. Lastly I also did all telemetry and server integration.\n3D UI/UX PROGRAMMING AND HARWARE ACCELLERATED RENDERING\nIn addition to programming all of the gameplay, I programmed the 3d GUI. the standards of fidelity for this UI system were quite high as there were many small details that were part of our client\u0026rsquo;s brand identity. I ensured rigorous compliance with GUI mocks even when I had to completely recreate certain effects by hand (like gaussian blur for certain GUI elements). I also made use of oculus\u0026rsquo;s hardware -accellerated gui rendering layer for crisper GUI elements.\nLOADING DYNAMIC CONTENT AT RUNTIME / STREAMING VIDEO\nI was able to dynamically load streaming video from the same server that powered the rest of the non-game learning material. This was an engineering constraint discovered mid-production cycle as we realized that shipping all video in the app would cause us to exceed oculus\u0026rsquo;s app store size limit- it was scary and initially seemed like a showstopper. I figured out how to manage proper formatting and resolution so that online video would also appear in the same high fidelity as the brand guidelines required.\nFACE AND MOTION CAPTURE MUXING\nI also created a system to dynamically mix facial capture performance with humanoid body animation so that it would match our content pipeline.\nLUA SCRIPTING\nall of dialoguesystem for unity\u0026rsquo;s content scripting database is in lua. I was able to rapidly learn enough lua to create content and pass important state info out to the rest of the game, as well as understanding some formatting quirks i would need to know to develop the tool pipeline\nTOOLS DEVELOPMENT AND CONTENT PIPELINE COORDINATION\nthis time, the import process from non-developer friendly narrative content systems like twine was fairly easy to get working with the game. I didn\u0026rsquo;t have to use regex again to parse and rewrite code which was a relief! I created some small editor scripts and directed the content team to use tools that would smoothly integrate with the pipeline- i also gave them special keywords to input that would parse their messages about the scenario into special lua commands so that they could control gamestate from their custom content without me having to write it for them.\n","date":"28 November 2024","externalUrl":null,"permalink":"/projects/1732755092786-sales-academy-vr/","section":"Projects","summary":"","title":"Sales Academy VR","type":"projects"},{"content":" Your browser does not support the video tag. Platform: PC and Mobile Engine: Unity Genre: Farming Sim, Geolocation AR game, Serious Game Team: 1 Engineer and Designer (me), 1 Data Scientist and UI artist, 1 team lead and design lead, 1 3d artist\nDescription: I volunteered with a group of concerned activists I met online to create a prototype for this environmental activism game. We later found a group within the IGDA and pitched to them unsuccessfully for funding, but not before creating the prototype seen here. The idea of the game was that environmental activist organizers could sign up on an external website and register their event location in the game. Then, activists who showed up in person to help with the environmental volunteering event, would receieve ingame rewards for attending. The game itself was a solarpunk-themed farming sim, and since the main limitation on the player would be the rare species of seeds they would earn from doing one-time environmental volunteer actions in real life, we also created a way to share your garden for other players to view.\nDesign Tasks:\nINTERNAL PITCHING AND MOCKUPS I worked to coordinate design efforts from the entire team, and leverage their individual capabilities. We defined our goals early on and were able to iterate quickly with mockups and small prototypes of ideas, which I completed several of while circling back with the team each time. I contributed some of my design ideas to what would appear in the final pitch and prototype, but more importantly, I was able to reconcile diverging design goals and ideas from different people on the team into something coherent.\nEngineering Tasks:\nGAMEPLAY SYSTEMS PROGRAMMING, LOADING DYNAMIC CONTENT FROM THE SERVER AT RUNTIME\nI used my existing experience pulling live server data into unity to create a prototype that would load geolocations and ingame event metadata into the game. We also scraped websites like the environmental voter scorecard and dynamically pulled information into the game from live-updated data for easy visualization and use in gameplay.\nBUGFIXING\nWe worked down to the wire to make sure that our gameplay experience was as ready for pitch time as possible. I was the sole QA specialist and maintainer of the game code, and I was working with the team to find and fix bugs at a rapid pace into the final hours. I was proud of myself for how well I handled this situation under pressure.\nOther Tasks:\nPITCH DECK AND PITCH SESSION\nI presented the pitch material along with our visionholder and team leader to a closed meeting with publishers who went on to play our prototype. She, the data scientist, and I also created the pitch deck together.\n","date":"28 November 2024","externalUrl":null,"permalink":"/projects/1732755164814-seeds-of-change/","section":"Projects","summary":"","title":"Seeds of Change","type":"projects"},{"content":" Your browser does not support the video tag. Platform: PC Engine: Unity Genre: Fighting Team: 1 solodev (me)\nDescription: This was my first major effort in creating an indie game of my own. I was inspired by the super smash bros community to create SQUAREFORCE, a flying fighting game where you mix-and-match moves on your spaceship to create a custom moveset. I designed, programmed, and did all the artwork for the game, as well as took it to several conferences where I tabled and talked to journalists and industry contacts to promote the game. I had dozens of people play my game at the various conferences I went to and local game developer events I regularly attended. In the end, this served to get me my first paid jobs in the industry, and I did not release the game, though at some point I still might (there are a lot of things I wish to improve about it)\nDesign Tasks:\nDESIGN DOCUMENTATION\nI created all the documentation by hand on physical paper- this was my first game and I didn\u0026rsquo;t yet know about personal knowledge management systems, formal structures and procedures for design documents, or mockup software. Instead I worked out the design of gameplay systems I wanted and how to technically and artistically achieve my aims. I didn\u0026rsquo;t stick to a specific small list of axioms for my goals, which is something I would do differently today. But in retrospect, I was basically trying to create a flying smash bros clone where you got to mix and match character normals. I also adopted spaceships as the characters early on, because I knew at the time that I wouldn\u0026rsquo;t be able to animate an entire roster of humanoid characters myself. Also, it gave an in-universe excuse for the characters to be sending chunks of energy at each other- another design goal I had was to make the hurtboxes of moves visible in the actual final art of the game so that it would be easier for new players to understand the properties of moves at a glance than with other fighting games. Over time I was able to leverage these constraints into a unique visual theme\nGAME SYSTEMS DESIGN\nI designed a number of game mechanics to compliment this vision. I really wanted to focus on accessibility because it was my view at the time that 2d fighting games were dwindling in popularity due to their steep learning curve and poor player communication. I made the movepool for each character very small (6) and made it so that you only needed to use 2 attack buttons and 4 face buttons total to play the game. I also was trying to design each type of input to also work with touch devices, but this was later scrapped. Many moves were only available through chording, which i thought would simplify the controls but actually it confused players. I also ran into some classic game engineering problems since it was my first project- one of which was the fact that my fast-moving attacks and ships would frequently clip through each other and terrain. I wrote a custom collision interpolation system to dynamically create meshes at runtime to interpolate between a hurtbox\u0026rsquo;s last frame and current frame. In keeping with my commitment to show the hurtboxes AS the artwork, i even considered coloring the runtime-created meshes so they would look like 3d motion smears (this concept was also being used by another independent smashlike project, earth romancer). In the end I did not use the smears for artistic purposes, but I used them to manage collision for most of the project\u0026rsquo;s lifetime until i eventually discovered established, simpler methods for continuous collision detection (i didn\u0026rsquo;t even know there was a word for it at the time)\nUX DESIGN\nI designed a 3d menu for the game that was a rough skeudomorphism of putting the ship parts into a sort of builder. I overscoped greatly because I couldn\u0026rsquo;t model all of the parts individually, but I was able to create a very tactile and satisfying menu system. I also responded to player confusion about input chording by making it so that in the menu, you have to input an attack in order to modify it on your loadout. This was a nice idea but in the end i think it made the menu controls too confusing. Other UI elements i was proud of the design of was the dash ring- I created a ring that expanded around the player while dash was held, and it would visually convey the dash \u0026lsquo;charging up\u0026rsquo; while also indicating exactly how far you would go if you buffered a turnaround. using VFX for player communication is something I wanted to do more of, but I liked that I at least made this. Also, through the use of my 3d skeudomorphic GUI, i was able to work around the problem where the unity version I was using at the time (4.x) was not capable of natively supporting multiple players using the same GUI eventSystem.\nCONTENT DESIGN AND GAME BALANCING\nI designed all the playable stages in the game to wrap in both the X and Y so that it was impossible to camp opponents in a corner. I wanted the \u0026lsquo;corner pressure\u0026rsquo; aspect of the fighting game to occur at the surfaces of the asteroids and other space objects floating in the stages. In the end this was a little confusing and a very big constraint on stage design, I\u0026rsquo;m not sure i would do it again if i made squareforce today. As for the moves themselves, it was very fun to create a lot of moves and think critically about how they could be used both in different kits, and in unconventional fighting game situations like continuously attacking below or above you. This was in addition to creating moves that obeyed the standards of fighting game content, such as fast pokes, combo starters, finishers, defensive attacks, and other well known tropes of fighting game move design.\nFOCUS TESTING\nI brought squareforce out to many different promotional events. I ran a booth at GDEX, an ohio convention where gamedevs from ohio and surrounding states could show their stuff. I also brought my game trailer out to GDC. I also got some professional smash players to try it at the cleveland esports summit! Most of all, I brought it out to local developer and enthusiast events in cleveland and Portland, Oregon many times (I was living in both places at the time). Each time I took notes on bugs players encountered, moments when they misunderstood what was happening in the game, common critiques, and negative possibility spaces/feature requests.\nEngineering Tasks:\nGAME SYSTEMS PROGRAMMING\nUI/UX PROGRAMMING\nPROJECT ARCHITECTURE\nCUSTOM COLLISION SYSTEM\nDYNAMIC ANIMATION REMAPPING\nbecause of the fact that my characters all used dynamically chosen attacks, I had to create a lot of animation control systems at runtime, much more than any similar project would. I used unity\u0026rsquo;s mecanim system and i made a generalized system of placeholders that could be overridden using an animatoroverridecontroller that I would create while the game was running. In retrospect, I wish I had known about unity playables or even some of the more basic code-only animation control systems available in the unity API. at the time though, I was very proud of the finite state machines I had built to drive character animation!\nENEMY AI\nI also used mecanim to drive enemy ai behavior in the game. Rather than use it for animations, i leveraged mecanim in an unusual way as a general FSM system to drive behavioral state changes.\nArt Tasks:\n3D OBJECT ART\nI created all of the 3d objects in the game myself! I chose a low poly style, and tried to adapt the aesthetic to my capabilities and what i knew I would be able to make. I was a novice in blender at the time, but I was able to create a variety of player ships and objects for the stages and 3d GUI screens. I consulted with my artist friends on how to apply character design principles to the ships, such as by adding contrast to important parts of the model and suggesting movement and action through the silouette and shape of the ships.\nANIMATION\nI also animated all of the attacks myself in addition to adding squash-and-stretch and \u0026lsquo;facial animations\u0026rsquo; to the ships! If i had kept going, I was planning for the lore of the game to involve a toy story-like premise in which some bizarre space physics made the ships slightly alive, but only while not being observed (in a nod to quantum physics\u0026rsquo;s wave-particle duality). I used blender shapekeys to animate all the facial expressions of the spaceships, and I used both blender and unity\u0026rsquo;s built-in animation editor to edit all of the squareforce-based attacks. I carefully studied other fighting games frame-by-frame to better understand concepts like key poses, contrast against background elements, anticipation, and visual communication of the impact of the hits- in addition to referencing the frame times of common archetypes of attacks for balance purposes.\nENVIRONMENT ART\nSince my art was low-poly, I relied a lot on lighting and VFX to visually distinguish my stages from one another. I made use of unity\u0026rsquo;s post-processing stack and experimented a lot with materials to visually define the objects in the stages. I wanted them to be atmospheric, but I also didn\u0026rsquo;t want them to hurt the player\u0026rsquo;s ability to see and understand what was going on. So I used very high contrast particle systems at the edges of certain objects to define them better, and I also would light the spaceships seperately with non-scene lighting in some of the stages. I also really enjoyed making all of the dust, plasma and explosion effects with unity shuriken!\nVFX ART\n3D UI ART\nOther Tasks:\nSOUND DESIGN\nPRODUCTION SCHEDULING AND MARKET RESEARCH\nWhile I was running the live promotion circuit for my game, I also managed my own social media presence, production schedule, and market research efforts. I collected sales data and other metadata about various types of fighting games to try and analyze my niche and understand a budget and release strategy. I also was briefly in talks with 2 publishers who helped me refine my market research and brand identity.\nPUBLISHER RELATIONS\nLIVE PROMOTION\nCOORDINATION WITH EXTERNAL CONTENT COMMISSIONING (musicians)\nThe only piece of Squareforce that I did not directly create myself was the music- I reached out to several musicians including popular youtube musician Argofox and an independent musician from the UK. Music from both of them appears in the game!\nSOCIAL MEDIA MARKETING\n","date":"28 November 2024","externalUrl":null,"permalink":"/projects/1732755208188-squareforce/","section":"Projects","summary":"","title":"Squareforce","type":"projects"},{"content":"","date":"28 November 2024","externalUrl":null,"permalink":"/tags/tag/","section":"Tags","summary":"","title":"Tag","type":"tags"},{"content":"","date":"28 November 2024","externalUrl":null,"permalink":"/tags/","section":"Tags","summary":"","title":"Tags","type":"tags"},{"content":" Your browser does not support the video tag. Platform: Oculus Quest Engine: Unity Genre: Educational, Simulation Team: 1 Developer (me) 1 3d artist, 1 project manager\nDescription: I worked on this game with Razoredge LLC, who in turn was contracting with a local university. The game was created to improve the process of training nursing staff on how to correctly handle radioactive cancer treatment drugs. the existing training method was costly and slow, as it involved calling a van out to the worksite and taking an entire day to train nursing staff with the real equipment. Furthermore, they would only get one chance to go through the experience. I\u0026rsquo;m confident that due to our creation of this serious game, they were able to recieve better, more accessible and more personalized training.\nEngineering Tasks:\nGAMEPLAY SYSTEMS PROGRAMMING\nI programmed all the gameplay in the game including internal state of the lesson, tracking player objects, and hand interactions such as parts rubber banding to their correct positions within certain tolerances.\nUNITY TIMELINE SEQUENCING\nUsing the same toolchain I was able to use from future of pet, I also authored all of the content in the game.\nEDITOR SCRIPTING\nI made minor edits to the toolchain to support animation related edge cases in the game.\nOther Tasks:\nART PIPELINE COORDINATION\nThe most involved part of this project was the animation of the small plastic tubing. While primarily a challenge for the artist to animate correctly, it was also troublesome to get it to align properly with the geometry over several stages of animation, and to apply bone weights to an obviously rope-like structure. We initially considered making a fully articulated physics rope system for this asset, but saw quickly that it would be more accurate to real life to portray the tubing assembly as unfurling in a pre-animated way over the course of several steps. I defined these steps with the artist and we worked together to make sure that it would look correct and convey learning information at each step of the animation.\n","date":"28 November 2024","externalUrl":null,"permalink":"/projects/1732755120568-therasphere-vr/","section":"Projects","summary":"","title":"Therasphere VR","type":"projects"},{"content":"an example to get you started\nCVS Virtual Clinic - Competency # Platform: Android, IOS, HTML5 Engine: Unity Genre: Educational, Dialogue based Team: 2 game developers(me), 1 web developer, 2 3d artists, 1 project lead, many subject matter experts and testers\nDescription: This is an educational game done in under contract with Case Western Reserve at the Francis Payne Bolton School of Nursing. It was also in partnership with CVS and the Institute for Healthcare Improvement (IHI). I was the design lead and a developer on this project. I currently also do maintenance on the game. Our goal in creating this game was to improve upon virtual clinic scenarios by lowering cognitive load and making gameplay more intuitive, so nurses could complete the material quicker and easier, and focus more on the learning material itself while playing. We also wanted to create an interface that would be more suitable to mobile devices.\nDesign Tasks:\nFIGMA MOCKUPS, LEADING DESIGN SESSION WITH STAKEHOLDERS\nI was much more prepared this time to deliver design insights to the visionholders and project lead. I took the lead in designing this version of the experience, and I presented all of my designs as mockups in figma or blender along with small prototypes of the gameplay systems that attracted the most stakeholder interest. I also led a design retreat where I guided all of the instructional design staff, SMEs, and project managers through brainstorming sessions about how things went with the first version of the virtual clinic and what we would want out of another such game, surfacing their most key concerns and insights and offering my best design responses backed up by evidence and rationale.\nINTERNAL PITCHING AND REVISION CYCLES\nI pitched 3 different prototypes after the design retreat. Once decided on a direction, I checked in repeatedly with the core team to ensure we were still aligned with their vision and what they wanted to do.\nINCORPORATION OF EDUCATION DESIGN TECHNIQUES\nI quickly began to align with and request additional time with the instructional design staff: I began to realize that these professionals are very similar to game designers in terms of what they are trained to do. I was also able to learn some instructional design words for contemporaneous concepts in game design, and it helped me communicate with the rest of the team better.\nUI DESIGN (ESPECIALLY THE SYMBOLS)\nOne key idea I wanted to emphasize was to create a strong visual shorthand for the types of age-related abnormalities that the nurses playing the game would be learning. Just like in other games, developing a consistent visual shape-language is great for player knowledge retention.\nSME CONSULTATION\nAs before, we checked in whenever we could with the SMEs to ensure we were still creating content that appropriately reflected the learning material and skills to be built by the playerbase.\nEngineering Tasks:\nUI PROGRAMMING\nwe designed the GUI systems in this version of the virtual clinic to be much more modular and easy to understand at a glance. We took extra care to ensure that a managable amount of relevant information was onscreen at any one time. I worked with my fellow game developer colleague to define the internal structure of the encounter in a more robust way that was ultimately more extensible than what we did in virtual clinic scenarios.\nCORE GAMEPLAY SYSTEMS\nEach core gameplay system was more strongly representative of a single line of inquiry that real nurses are meant to follow while going through the process of diagnosing age-related abnormalities. Focusing very strongly on interactions that were easy to input and easy to understand at a glance, I created a modular camera and GUI \u0026lsquo;docking\u0026rsquo; system where modular stages of gameplay could request sections of screenspace to use at a time.\nWEB PLATFORM DEPLOYMENT AND OPTIMIZATION\nHaving a better understanding of what platforms we intended to ship to this time, we were better able to optimize polycount and texture memory for both web and mobile. I was also able to handle more of the web deployment infrastructure myself this time.\nMOBILE PLATFORM DEPLOYMENT AND OPTIMIZATION\nwe also deployed cross-platform to ios and android. Many of our interactions were clear and simple enough to translate easily with minor tweaking- however, we were not so lucky with accessibility. We were required by internal rules at case to equip any software we released with appropriate screenreader accessibility features for the visually impaired. While the specifics of these requirements did not seem designed for games, we followed them as best as we could. My fellow gamedev heroically did a lot of the work in moving both apple\u0026rsquo;s core screenreader nodes as well as android\u0026rsquo;s, but I helped. Unity\u0026rsquo;s more advanced screenreader support did not roll out until unity 2023, so we had to retrofit and roll all of this functionality ourselves- it was a big lift!\nACCESSIBILITY LIBRARY WITH ANDROID AND IOS\nCUSTOM SPLINE-BASED CHARACTER MOVEMENT SYSTEM\nWe created a simplified character movement system for the small characters in the 3d scene section of the game. I learned a lot about spline math from my developer friend, who helped me port a generalized spline system into unity from a separate software project and adapt it to work with our custom animation framework (whose state was driven by the dialogues we created the archtecture for as well).\nCINEMACHINE FALSE PERSPECTIVE SYSTEM\nI incorporated unity\u0026rsquo;s cinemachine into my screen element docking system, and utilized false perspective to make it so that certain 3d scene elements would appear in a similar manner to gui objects.\nLOADING CONTENT DYNAMICALLY FROM SERVER\nas was the case last time, we were loading dynamic content at runtime from our game\u0026rsquo;s server, and I programmed the data ingestion into unity as before. this time we got a lot more activity from the SMEs who were able to dynamically submit changes to our server and experience live updates without having to redeploy the app!\nOther Tasks:\nHIRING AND MANAGING ART STAFF\nTragically, the artist on our team from the previous game passed away between development of the versions of the virtual clinic. After taking time to grieve, it was my responsibility to find a way to get new artwork done quickly as the college\u0026rsquo;s formal hiring process would take too long before release of the game. I utilized my company and subcontracted 2 3d artists from my career network. In so doing, I got some experience managing employees, handling their taxes and paperwork, and delegating responsibilities to them. All of my previous struggles defining art pipelines paid off, as I feel I was able to communicate project requirements and scope to them very clearly and get great results.\nART CLEANUP\nIn the interim time, I was able to do a bit better than the standard \u0026lsquo;programmer artwork.\u0026rsquo; Using Blender, I was able to create the 3d environment that was ultimately used in the final game. I used what I learned from the previous version of the game to do texture atlassing to save memory.\nSCIENTIFIC WRITING ASSISTANCE\nAs with the previous version of the virtual clinic, I once again helped the data science team to compile results for their formal scientific journal article. I helped to write some of the sections of the paper.\nPUBLIC SPEAKING AND TABLING AT CONFERENCES\nWe showed this version of the virtual clinic at the 2022 serious play conference in Florida. Myself, the other game developer, and the web developer all went on the trip together and delivered our results along with gameplay and design rationale in a conference talk to the audience. It was great to meet other devs of serious games, and we were honored to have our game featured in the conference speaker lineup!\n","date":"28 November 2024","externalUrl":null,"permalink":"/projects/1732755052324-virtual-clinic-competency/","section":"Projects","summary":"","title":"Virtual Clinic Competency","type":"projects"},{"content":"(/ezgif-2-cd702af56367.webp)\nPlatform: HTML5 Engine: Unity Genre: Educational, dialogue based Team: 2 game developers(me), 1 web developer, 1 project manager, 3 3d artists, 1 project lead, many subject matter experts and testers\nDescription: This is an educational game done in under contract with Case Western Reserve at the Francis Payne Bolton School of Nursing. It was also in partnership with CVS and the Institute for Healthcare Improvement (IHI). I was a designer and developer on this project. I currently also do maintenance on the game. Our goal in creating the game was to produce an intuitive learning environment where nurses learn to deliver better geriatric care in Minuteclinics around the US, by using an approach to elder care called the 4 M\u0026rsquo;s system.\nDesign Tasks:\nI was working on the team with someone else from the Cleveland game developer scene and we pitched the idea that the learning platform could be a game. After stakeholder buy-in, we began meeting with Subject matter experts (SME\u0026rsquo;s) to hash out the design of the game. I shared design responsibilities with the other game developer and web developer, and we all participated in whiteboarding sessions to define a design for the game that would meet the learning goals defined by the SMEs and project oversight.\nWHITEBOARDING SESSIONS\nI led a series of whiteboarding sessions with the development team, core stakeholders, and 2 SMEs. we whiteboarded possible screen layouts for each prototype as well as mind maps of the most important learning material and how it was structured according to the SMEs. this was early in my career, if i were to lead such a meeting again today, I would probably use Miro to outline the learning goals and figma to mock screenshots\nSME CONSULTATION\nI initially believed we were replacing existing didactic learning material with a game, but actually a lot of the learning goals had not been defined yet for the learning intervention itself. We talked about what the difference in competencies would be between a hypothetical player who comes into the program, and one who has played the game and knows the skills. As stated, this was accomplished both in whiteboard meetings at the beginning of the project, but also through checking in with the SMEs when they were available. sometimes they disagreed on the material, and in this case it was left to the project stakeholder to give the final say.\nPAPER PROTOTYPING\nFor later prototypes we also advanced to paper prototyping to give stakeholders a better idea of what we were creating. this facilitated more rapid iteration on the way that players would best engage with the learning material in gameplay.\nWHITEBOXING\nI created all the level geometry in the early levels before it was time to do a more detailed pass with an artist. I also whiteboxed all the prototypes that preceded the final version of Scenarios, which included some very different types of gameplay including a character creator and isometric office view\nUSER TESTING\nI coordinated playtest sessions with nurse practitioners at the college to collect their thoughts on the gameplay and what should be improved. I told other observers what to watch for and take notes on while we physically observed people play the game.\nUI DESIGN\nOnce we got the go-ahead to adapt our third prototype into the full game, the other game dev on the staff and I both worked on many different versions of the GUI for the game. We created several versions of the radial menu seen here, and iterated on which ones would best show relationships between the content without explicitly telling the player. We used rules of graphic design and composition to best select what shapes and colors to use to help players form associations implicitly. we also created a \u0026lsquo;mini map\u0026rsquo; on the scrollbar of the encounter text so the player has a visual sense of where and how much of the encounter has been modified by their actions.\nREDESIGNS\nWe did 3 prototypes of the game - one of which was a first - person game where you click visible signs of mobility issues on the patient, one where you create a patient as a puzzle for other players to solve, and one where you step through the encounter changing the language to be more age friendly. since the first one was needlessly timing-based, and coupled strongly to only focus on mobility issues (not medication ones for instance, that couldn\u0026rsquo;t be easily seen in 3d), and since the second was explosively unbounded in terms of the content players would be able to create and we weren\u0026rsquo;t sure if that would be accurate for educational purposes at that point, we chose our 3rd prototype to develop into the final game\nART PIPELINE DEFINITION\nWe had many issues with the format, exposed design parameters, and detail level of the humanoid animations in the game. I took it upon myself to work with the artist on the team to more carefully define standards for how the 3d motion data should be formatted to help improve process flow during development.\nART STYLE DEVELOPMENT\nThe other gamedev, artist, and myself carefully chose a visual style for our game based on usability and cultural proximity to similar types of learning material. at first, we made abstract, color-field like characters and environments to cut down on visual complexity and evoke the look of a physical safety card. eventually though, we were asked by stakeholders to change to a look that focused on realism, presumably to better match video training material.\nEngineering Tasks:\nUI PROGRAMMING\nThe other gamedev and I did all the programming of the UI elements we designed. It was important to strike a balance between responsive animated ui and ui that would stay in the same place while being interacted with. also we custom-created a lot of radial layout functionality and nonstandard button colliders to match our initial ui design.\nCORE GAMEPLAY SYSTEMS\nIn addition to the ui programming, we also programmed all of the camera, character agent, scenario state management, and remote server infrastructure for the game. We initially thought that SMEs would be continuously wanting to add new content, so we created a CRUD system to help non-programmers edit ingame patient scenarios and load them ingame. ultimately this went unused, but i still learned a lot about server-based content loading in unity.\nMOCAP CUSTOM HARDWARE CONFIGURATION\nWe set up a custom impromptu mocap studio at the college using a series of off-the-shelf parts such as XB1 kinect cameras. Using open source tools, we captured 3d motion data from several actors and medical experts who knew how to do the irregular walking gait that would be featured in the game. We used some of this animation, but it was also very messy and low-fidelity, so the artist manually created some other humanoid animations.\nMOCAP DATA CLEANUP\nI helped the artist do a bit of the cleanup of the captured animation. IK of the feet was a bit messy and there were a lot of excess keyframes. in the end it was mostly useful as a block-out for the handcrafted animations the artist made.\n3D ANIMATION BLENDING\nI scripted all of the blending of the 3d animations in the game using unity\u0026rsquo;s mecanim state machine. I also manually programmed some of the pathing for the characters\nFSM MANAGEMENT\nthere were a few other state machine driven systems in the game. due to having just worked on squareforce before this game, I was used to unity\u0026rsquo;s FSM/mecanim system and did every state machine in the project myself.\nLOADING DYNAMIC CONTENT FROM THE SERVER AT RUNTIME\nAs previously mentioned, scenario data including text and animation sequences were loaded dynamically at runtime. At this point in my career I hadn\u0026rsquo;t done any server management yet, so my fellow gamedev programmed the API endpoint while I programmed the content ingestion on the unity side.\nCRUD CONTENT PIPELINE SETUP\nI created a visual CRUD application to constrain input to make sure that non-technical users could input valid scenario changes without breaking the game. ultimately the SMEs were not available to do this for most of the project\u0026rsquo;s lifecycle, and we ended up just filling in most of the content ourselves anyway.\nWEB PLATFORM DEPLOYMENT AND OPTIMIZATION\nWe deployed to a number of tightly constrained systems over the course of the game\u0026rsquo;s life cycle. we initially deployed desktop builds, but switched to web builds due to internal legal restrictions at our client\u0026rsquo;s worksite. This was a challenge as the web platform was much more memory constrained and difficult to optimize for, and also came with unseen gotchas like web rendering systems only respecting a lower number of bone weights, meaning some animation had to be redone. The three of us (2 gamedevs and artist) created a system to asynchronously load all of the character\u0026rsquo;s textures, so by doing that and atlassing/downrezzing some others, we were able to improve the performance of the game on these machines.\nTELEMETRY\nWe also programmed a lot of telemetry into the game to report certain ingame actions from the player to the server. My gamedev colleague did most of this, but I helped and was able to do similarly complex tasks on our next project together.\nOther Tasks:\nSCIENTIFIC WRITING ASSISTANCE\nI made use of my former life as a science professional to help SME\u0026rsquo;s and the data team to summarize/contextualize our efforts in the schema of study design and formal paper formatting. I also helped to add my perspective as a game developer to the discussion and analysis components of the paper.\nPUBLIC SPEAKING AND TABLING AT CONFERENCES\nWhen showing our prototypes at the QSEN medical conference, I was chosen by the team to deliver a short speech about our efforts. I like public speaking and summarizing scientific topics, so I happily volunteered. It was nice to meet so many people!\nVOICE ACTING\nI also provided VO for supplemental materials on the project including video tutorials for the game and other aspects of the learning intervention. I tried to adopt a subdued and welcoming tone similar to an educational segment narrator. I also set up a small recording area in my home office with some audio foam, and I cleaned and cut all the audio myself using audacity on linux.\n","date":"28 November 2024","externalUrl":null,"permalink":"/projects/1732755038399-virtual-clinic-scenarios/","section":"Projects","summary":"","title":"Virtual Clinic Scenarios","type":"projects"},{"content":"","date":"28 November 2024","externalUrl":null,"permalink":"/tags/web/","section":"Tags","summary":"","title":"Web","type":"tags"},{"content":"","externalUrl":null,"permalink":"/authors/","section":"Authors","summary":"","title":"Authors","type":"authors"},{"content":"","externalUrl":null,"permalink":"/categories/","section":"Categories","summary":"","title":"Categories","type":"categories"},{"content":"","externalUrl":null,"permalink":"/series/","section":"Series","summary":"","title":"Series","type":"series"}]